"team_name": "4-pack" # Your team name
"eval_method": ["mcqa", "rag"] # mcqa, reward, rag, quantiz
"task_type": "causal_lm" # causal_lm, seq2seq
"policy_model_path": "PeterAM4/EPFL-TA-Meister" # Your path to the final checkpoint
"reference_model_path": "meta-llama/Meta-Llama-3-8B" # The repo id of your pretrained reference model
"quantized_policy_model_path": "PeterAM4/EPFL-TA-Meister-8bit" # Your path to the final quantized checkpoint
"rag_policy_model_path": "PeterAM4/EPFL-TA-Meister" # Your path to the final RAG checkpoint
"test_data_path": "./datasets/mcqa_final_small.jsonl"  # "./data/mmlu.jsonl" # Your path to the test data
"dpo_model_args": # Put any model arguments required to load your DPO model below
  "device_map": "cuda"
  "device": "cuda"

"rag_model_args": # Put any model arguments required to load your rag model below
  "embedding_model_path": "BAAI/bge-base-en-v1.5"
  "index_path": "./documents/db4"
  "device_map": "cuda"
  "device": "cuda"

"quantized_model_args": # Put any model arguments required to load your quantized model below
  "device_map": "cuda"
  "device": "cuda"
