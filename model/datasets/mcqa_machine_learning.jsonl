{"subject": "machine_learning", "question": "Question: _ refers to a model that can neither model the training data nor generalize to new data.\n\nOptions:\nA. good fitting\nB. overfitting\nC. underfitting\nD. all of the above\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: K-fold cross-validation is\n\nOptions:\nA. linear in K\nB. quadratic in K\nC. cubic in K\nD. exponential in K\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n\nOptions:\nA. 0\nB. 1\nC. 2\nD. 3\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: After applying a regularization penalty in linear regression, you find that some of the coefficients of w are zeroed out. Which of the following penalties might have been used?\n\nOptions:\nA. L0 norm\nB. L1 norm\nC. L2 norm\nD. either (a) or (b)\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z?\n\nOptions:\nA. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\nB. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y)\nC. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z)\nD. P(X, Y, Z) = P(X) * P(Y) * P(Z)\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Which is true about Batch Normalization?\n\nOptions:\nA. After applying batch normalization, the layer\u2019s activations will follow a standard Gaussian distribution.\nB. The bias parameter of affine layers becomes redundant if a batch normalization layer follows immediately afterward.\nC. The standard weight initialization must be changed when using Batch Normalization.\nD. Batch Normalization is equivalent to Layer Normalization for convolutional neural networks.\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Which of the following is a clustering algorithm in machine learning?\n\nOptions:\nA. Expectation Maximization\nB. CART\nC. Gaussian Na\u00efve Bayes\nD. Apriori\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Statement 1| The Stanford Sentiment Treebank contained movie reviews, not book reviews. Statement 2| The Penn Treebank has been used for language modeling.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?\n\nOptions:\nA. O(D)\nB. O(N)\nC. O(ND)\nD. O(ND^2)\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Statement 1| Maximizing the likelihood of logistic regression model yields multiple local optimums. Statement 2| No classifier can do better than a naive Bayes classifier if the distribution of the data is known.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Which of the following points would Bayesians and frequentists disagree on?\n\nOptions:\nA. The use of a non-Gaussian noise model in probabilistic regression.\nB. The use of probabilistic modelling for regression.\nC. The use of prior distributions on the parameters in a probabilistic model.\nD. The use of class priors in Gaussian Discriminant Analysis.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| The softmax function is commonly used in mutliclass logistic regression. Statement 2| The temperature of a nonuniform softmax distribution affects its entropy.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: You are reviewing papers for the World\u2019s Fanciest Machine Learning Conference, and you see submissions with the following claims. Which ones would you consider accepting? \n\nOptions:\nA. My method achieves a training error lower than all previous methods!\nB. My method achieves a test error lower than all previous methods! (Footnote: When regularisation parameter \u03bb is chosen so as to minimise test error.)\nC. My method achieves a test error lower than all previous methods! (Footnote: When regularisation parameter \u03bb is chosen so as to minimise cross-validaton error.)\nD. My method achieves a cross-validation error lower than all previous methods! (Footnote: When regularisation parameter \u03bb is chosen so as to minimise cross-validaton error.)\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: If N is the number of instances in the training dataset, nearest neighbors has a classification run time of\n\nOptions:\nA. O(1)\nB. O( N )\nC. O(log N )\nD. O( N^2 )\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Statement 1| The set of all rectangles in the 2D plane (which includes non axisaligned rectangles) can shatter a set of 5 points. Statement 2| The VC-dimension of k-Nearest Neighbour classifier when k = 1 is infinite.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| Word2Vec parameters were not initialized using a Restricted Boltzman Machine. Statement 2| The tanh function is a nonlinear activation function.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Statement 1| Traditional machine learning results assume that the train and test sets are independent and identically distributed. Statement 2| In 2017, COCO models were usually pretrained on ImageNet.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Which of the following is true of a convolution kernel?\n\nOptions:\nA. Convolving an image with $\\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ would not change the image\nB. Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image\nC. Convolving an image with $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ would not change the image\nD. Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: If your training loss increases with number of epochs, which of the following could be a possible issue with the learning process?\n\nOptions:\nA. Regularization is too low and model is overfitting\nB. Regularization is too high and model is underfitting\nC. Step size is too large\nD. Step size is too small\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Another term for out-of-distribution detection is?\n\nOptions:\nA. anomaly detection\nB. one-class detection\nC. train-test mismatch robustness\nD. background detection\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Averaging the output of multiple decision trees helps _.\n\nOptions:\nA. Increase bias\nB. Decrease bias\nC. Increase variance\nD. Decrease variance\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Statement 1| For any two variables x and y having joint distribution p(x, y), we always have H[x, y] \u2265 H[x] + H[y] where H is entropy function. Statement 2| For some directed graphs, moralization decreases the number of edges present in the graph.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Statement 1| Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions. Statement 2| DenseNets usually cost more memory than ResNets.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Statement 1| The original ResNets and Transformers are feedforward neural networks. Statement 2| The original Transformers use self-attention, but the original ResNet does not.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Which of the following can only be used when training data are linearly separable?\n\nOptions:\nA. Linear hard-margin SVM.\nB. Linear Logistic Regression.\nC. Linear Soft margin SVM.\nD. The centroid method.\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: When doing least-squares regression with regularisation (assuming that the optimisation can be done exactly), increasing the value of the regularisation parameter \u03bb the testing error.\n\nOptions:\nA. will never decrease the training error.\nB. will never increase the training error.\nC. will never decrease the testing error.\nD. will never increase\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Statement 1| RELUs are not monotonic, but sigmoids are monotonic. Statement 2| Neural networks trained with gradient descent with high probability converge to the global optimum.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Which of the following is false?\n\nOptions:\nA. The following fully connected network without activation functions is linear: $g_3(g_2(g_1(x)))$, where $g_i(x) = W_i x$ and $W_i$ are matrices.\nB. Leaky ReLU $\\max\\{0.01x,x\\}$ is convex.\nC. A combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is convex.\nD. The loss $\\log \\sigma(x)= -\\log(1+e^{-x})$ is concave\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| A neural network's convergence depends on the learning rate. Statement 2| Dropout multiplies randomly chosen activation values by zero.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Consider the Bayesian network given below. How many independent parameters would we need if we made no assumptions about independence or conditional independence H -> U <- P <- W?\n\nOptions:\nA. 3\nB. 4\nC. 7\nD. 15\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Suppose your model is overfitting. Which of the following is NOT a valid way to try and reduce the overfitting?\n\nOptions:\nA. Increase the amount of training data.\nB. Improve the optimisation algorithm being used for error minimisation.\nC. Decrease the model complexity.\nD. Reduce the noise in the training data.\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Neural networks:\n\nOptions:\nA. Optimize a convex objective function\nB. Can only be trained with stochastic gradient descent\nC. Can use a mix of different activation functions\nD. None of the above\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Which one of the following is the main reason for pruning a Decision Tree?\n\nOptions:\nA. To save computing time during testing\nB. To save space for storing the Decision Tree\nC. To make the training set error smaller\nD. To avoid overfitting the training set\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Which of the following is false?\n\nOptions:\nA. Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.\nB. A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive.\nC. When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.\nD. A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: What is the dimensionality of the null space of the following matrix? A = [[3, 2, \u22129], [\u22126, \u22124, 18], [12, 8, \u221236]]\n\nOptions:\nA. 0\nB. 1\nC. 2\nD. 3\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x).\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?\n\nOptions:\nA. 12\nB. 24\nC. 48\nD. 72\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Which of the following sentence is FALSE regarding regression?\n\nOptions:\nA. It relates inputs to outputs.\nB. It is used for prediction.\nC. It may be used for interpretation.\nD. It discovers causal relationships\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: What are support vectors?\n\nOptions:\nA. The examples farthest from the decision boundary.\nB. The only examples necessary to compute f(x) in an SVM.\nC. The data centroid.\nD. All the examples that have a non-zero weight \u03b1k in a SVM.\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Statement 1| The maximum margin decision boundaries that support vector machines construct have the lowest generalization error among all linear classifiers. Statement 2| Any decision boundary that we get from a generative model with classconditional Gaussian distributions could in principle be reproduced with an SVM and a polynomial kernel of degree less than or equal to three.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: The numerical output of a sigmoid node in a neural network:\n\nOptions:\nA. Is unbounded, encompassing all real numbers.\nB. Is unbounded, encompassing all integers.\nC. Is bounded between 0 and 1.\nD. Is bounded between -1 and 1.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Suppose we like to calculate P(H|E, F) and we have no conditional independence information. Which of the following sets of numbers are sufficient for the calculation?\n\nOptions:\nA. P(E, F), P(H), P(E|H), P(F|H)\nB. P(E, F), P(H), P(E, F|H)\nC. P(H), P(E|H), P(F|H)\nD. P(E, F), P(E|H), P(F|H)\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Given a Neural Net with N input nodes, no hidden layers, one output node, with Entropy Loss and Sigmoid Activation Functions, which of the following algorithms (with the proper hyper-parameters and initialization) can be used to find the global optimum?\n\nOptions:\nA. Stochastic Gradient Descent\nB. Mini-Batch Gradient Descent\nC. Batch Gradient Descent\nD. All of the above\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Statement 1| The derivative of the sigmoid $\\sigma(x)=(1+e^{-x})^{-1}$ with respect to $x$ is equal to $\\text{Var}(B)$ where $B\\sim \\text{Bern}(\\sigma(x))$ is a Bernoulli random variable. Statement 2| Setting the bias parameters in each layer of neural network to 0 changes the bias-variance trade-off such that the model's variance increases and the model's bias decreases\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| ImageNet has images of various resolutions. Statement 2| Caltech-101 has more images than ImageNet.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\nOptions:\nA. 2\nB. 4\nC. 8\nD. 16\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Which image data augmentation is most common for natural images?\n\nOptions:\nA. random crop and horizontal flip\nB. random crop and vertical flip\nC. posterization\nD. dithering\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during\n\nOptions:\nA. Best-subset selection\nB. Forward stepwise selection\nC. Forward stage wise selection\nD. All of the above\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________\n\nOptions:\nA. higher\nB. same\nC. lower\nD. it could be any of the above\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Statement 1| Overfitting is more likely when the set of training data is small. Statement 2| Overfitting is more likely when the hypothesis space is small.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Existential risks posed by AI are most commonly associated with which of the following professors?\n\nOptions:\nA. Nando de Frietas\nB. Yann LeCun\nC. Stuart Russell\nD. Jitendra Malik\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: High entropy means that the partitions in classification are\n\nOptions:\nA. pure\nB. not pure\nC. useful\nD. useless\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Statement 1| The SVM learning algorithm is guaranteed to find the globally optimal hypothesis with respect to its object function. Statement 2| After being mapped into feature space Q through a radial basis kernel function, a Perceptron may be able to achieve better classification performance than in its original space (though we can\u2019t guarantee this).\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:", "answer": "A"}
